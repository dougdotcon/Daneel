Perfeito. Vou investigar a fundo como agentes IA de codificação como Augment, Cursor, Continue e outros são construídos, com foco especial no desempenho no benchmark SWE-bench. Vou detalhar as ferramentas, ajustes finos, estratégias de prompting, arquiteturas multi-agente e modelos de linguagem que ajudam a alcançar alta performance em tarefas de engenharia de software.

O objetivo será produzir um plano passo a passo para você construir um agente local-first (Node.js) com suporte a APIs externas, que supere 60% no SWE-bench. Assim que o material estiver pronto, te aviso.

# Construindo um Agente IA de Codificação no VSCode – Guia Técnico Avançado

Desenvolver uma extensão VSCode com um agente de IA de codificação **local-first** (priorizando execução local) e capaz de recorrer a APIs externas requer uma arquitetura robusta e estratégias bem definidas. A seguir, apresentamos um guia passo a passo abordando os principais componentes e técnicas necessárias para criar um agente de alta performance, visando superar a marca de **60% de acurácia no benchmark SWE-bench**. Serão discutidos desde a arquitetura e prompting até ferramentas, avaliação contínua e uso de multi-agentes especializados. 

## 1. Arquitetura Recomendada do Agente

Um agente de codificação integrado ao VSCode consiste em diversos componentes que trabalham em conjunto. A arquitetura básica inclui:

- **Extensão VSCode (Frontend)**: Interface que interage com o usuário (via chat ou comandos) e exibe sugestões/ações do agente.
- **Orquestrador do Agente (Backend)**: Módulo em Node.js que coordena o fluxo de trabalho do agente. Ele envia prompts ao modelo de linguagem, interpreta as respostas e decide chamadas de ferramentas (e.g. ler/escrever arquivos, executar testes).
- **Modelo(s) de Linguagem (LLM)**: O “cérebro” do agente. Pode ser um LLM executado localmente (ex: variantes open-source do Llama) ou via API externa (OpenAI, Anthropic, etc.), dependendo da necessidade. É recomendável permitir flexibilidade para usar vários provedores – por exemplo, rodar um modelo local por padrão e, opcionalmente, fazer upgrade para um modelo via API para tarefas complexas. Ferramentas como a extensão AgentSmith já suportam múltiplos provedores (OpenAI, Cohere, Anthropic **e até modelos locais via Ollama**) dentro de uma mesma arquitetura ([
        Agent Smith - Visual Studio Marketplace
    ](https://marketplace.visualstudio.com/items?itemName=nr-codetools.agentsmith#:~:text=Powerful%20AI%20Models%3A%20Supports%20all,AI%2C%20Gemini%2C%20Cohere%2C%20Anthropic%2C%20Ollama)).
- **Ferramentas (Tools)**: Conjunto de utilitários que o agente pode invocar para atuar sobre o ambiente de desenvolvimento. Exemplos: leitura e edição de arquivos do projeto, busca de texto no código, execução de comandos no terminal (para rodar testes ou ferramentas de build), fetch de dados externos, etc. O VSCode Agent Mode já provê ferramentas internas como `read_file`, `edit_file` e `run_in_terminal`, e permite adicionar ferramentas customizadas ([Agent mode: available to all users and supports MCP](https://code.visualstudio.com/blogs/2025/04/07/agentMode#:~:text=%2A%20Built,green%20in%20the%20diagram)). Essas ferramentas expõem capacidades que o LLM sozinho não tem, permitindo que o agente navegue e modifique o workspace.
- **Contexto de Código e Memória**: Mecanismo para fornecer ao agente informações de contexto do projeto (p. ex., conteúdo de arquivos relevantes, estrutura do repositório) sem exceder limites de token do modelo. Isso inclui um índice da base de código (via embeddings ou outros métodos) e possivelmente memória de interações prévias ou preferências do usuário.

 ([Agent mode: available to all users and supports MCP](https://code.visualstudio.com/blogs/2025/04/07/agentMode)) *Visão geral da arquitetura de um agente de codificação integrado ao VSCode, ilustrando a interação entre usuário, agente (LLM) e ferramentas no workspace.* ([Agent mode: available to all users and supports MCP](https://code.visualstudio.com/blogs/2025/04/07/agentMode#:~:text=%2A%20Built,green%20in%20the%20diagram))

Na prática, a extensão VSCode atuará como **cliente** que captura pedidos do usuário (ex.: *“Corrija o bug X na função Y”*) e envia para o **orquestrador do agente**. Este orquestrador prepara um *prompt* para o modelo contendo a descrição da tarefa e o contexto inicial necessário (arquivos relevantes, instruções especiais, etc.). O **ciclo de inferência** do agente então se desenrola em loop: o modelo propõe ações ou soluções, o agente executa as ações (usando ferramentas) e observa os resultados, alimentando novamente o modelo até concluir a tarefa. Em outras palavras, o agente funciona de forma autônoma semelhante a um par programador inteligente: ele **planeja e executa passos para cumprir a solicitação**, determinando quais arquivos são relevantes, aplicando modificações no código, sugerindo comandos no terminal e iterando conforme necessário até resolver o problema ([VS Code v1.99 Is All About Copilot Chat AI, Including Agent Mode -- Visual Studio Magazine](https://visualstudiomagazine.com/Articles/2025/04/04/VS-Code-v1-99-Is-All-About-Copilot-Chat.aspx#:~:text=Agent%20Mode%20offers%20an%20autonomous,tools%2C%20applications%2C%20and%20data%20sources)) ([VS Code v1.99 Is All About Copilot Chat AI, Including Agent Mode -- Visual Studio Magazine](https://visualstudiomagazine.com/Articles/2025/04/04/VS-Code-v1-99-Is-All-About-Copilot-Chat.aspx#:~:text=,documentation)). Durante esse processo, o desenvolvedor permanece no controle aprovando ou revisando as mudanças sugeridas.

**Local vs. Nuvem:** Para atender ao requisito *local-first*, o agente deve utilizar um modelo local sempre que possível e somente recorrer a APIs externas quando precisar de maior poder. Isso implica incluir no design um componente de seleção de modelo: por exemplo, usar um *LLM open-source fine-tunado local* para a maioria das sugestões, e opcionalmente delegar a chamada a um modelo SaaS (como Claude ou GPT-4) caso o usuário solicite uma segunda opinião ou para problemas muito complexos. Frameworks como o Continue adotam arquitetura aberta que facilita essa intercambialidade – ele conecta diversos provedores e servidores de modelo, evitando travar o usuário a um só backend ([Launch HN: Continue (YC S23) – Create custom AI code assistants | Hacker News](https://news.ycombinator.com/item?id=43494427#:~:text=The%20AI,locking%20you%20into%20yesterday%27s%20technology)) ([Launch HN: Continue (YC S23) – Create custom AI code assistants | Hacker News](https://news.ycombinator.com/item?id=43494427#:~:text=servers%2C%20assistant%20rules%2C%20etc,locking%20you%20into%20yesterday%27s%20technology)). No seu agente, você pode implementar algo semelhante permitindo configurar no **config** da extensão qual modelo usar para cada tipo de tarefa ou um fallback hierárquico (ex.: “tente primeiro `llama2-13b` local, se falhar ou for muito impreciso, chame API do Claude Sonnet 3.7”). 

Outro aspecto arquitetural importante é definir **como o VSCode vai se comunicar com o agente**. Duas abordagens comuns são: 
- **Implementar o agente totalmente dentro da extensão** (em Node.js), usando as APIs do VSCode para acessar arquivos e executar comandos. Por exemplo, usando Node child_process para rodar testes, ou FileSystem API para ler/escrever código. 
- **Implementar um servidor separado** (por exemplo, um servidor Language Server Protocol ou um servidor custom via sockets/HTTP) que roda o agente, e a extensão VSCode atua como cliente. O uso do `vscode-languageserver` (LSP) pode ajudar se você quiser aproveitar funcionalidades padrão de linguagem (diagnósticos, indexing), mas para um agente coding geralmente um servidor custom via STDIO/SSE seguindo o protocolo **MCP (Model Context Protocol)** é mais apropriado ([Agent mode: available to all users and supports MCP](https://code.visualstudio.com/blogs/2025/04/07/agentMode#:~:text=When%20the%20VS%20Code%20team,circle%20back%20to%20VS%20Code)) ([Agent mode: available to all users and supports MCP](https://code.visualstudio.com/blogs/2025/04/07/agentMode#:~:text=input%20and%20follow%20the%20Add,sent)). O MCP é um novo protocolo (inspirado no LSP) que padroniza a comunicação de contexto e ferramentas entre IDEs e agentes de LLM ([Agent mode: available to all users and supports MCP](https://code.visualstudio.com/blogs/2025/04/07/agentMode#:~:text=When%20the%20VS%20Code%20team,circle%20back%20to%20VS%20Code)). VSCode já suporta conectar a servidores MCP locais, então você pode implementar seu agente como um processo MCP (por exemplo, um script Node ou Python rodando separadamente) e registrar suas ferramentas.

**Resumindo os componentes e interações:** o usuário faz um pedido em linguagem natural, o agente (modelo de linguagem + lógica de orquestração) recebe esse *prompt* junto com o contexto do workspace, então possivelmente utiliza ferramentas (ler arquivos, buscar erros, etc.) e retorna gradualmente uma solução. Esse resultado pode ser um conjunto de mudanças de código aplicadas automaticamente ou sugestões exibidas para o desenvolvedor aprovar. Todo o processo deve preservar a segurança e controle: idealmente, o agente pede confirmação do usuário antes de executar ações destrutivas (como rodar um comando ou efetuar mudanças extensas) ([Agent mode: available to all users and supports MCP](https://code.visualstudio.com/blogs/2025/04/07/agentMode#:~:text=Image%3A%20Edit%20UI%20showing%20how,to%20enable%20and%20disable%20tools)) ([Agent mode: available to all users and supports MCP](https://code.visualstudio.com/blogs/2025/04/07/agentMode#:~:text=To%20give%20you%20full%20control%2C,remote%20if%20you%20allow%20it)).

## 2. Estratégias de **Prompting** e **Chaining** para Tarefas Complexas

Para que o agente seja eficaz em tarefas complexas de engenharia de software, é fundamental moldar o comportamento do LLM por meio de **prompts estruturados** e possivelmente dividir a tarefa em sub-etapas encadeadas (*chain-of-thought*). Ferramentas avançadas como Augment e Anthropic Claude demonstraram que fornecer uma instrução inicial bem detalhada e passo-a-passo aumenta muito a taxa de sucesso em resolver problemas reais ([#1 open-source agent on SWE-Bench Verified by combining Claude 3.7 and O1](https://www.augmentcode.com/blog/1-open-source-agent-on-swe-bench-verified-by-combining-claude-3-7-and-o1#:~:text=1,that%20the%20error%20is%20fixed)).

Uma abordagem recomendada é começar cada sessão do agente com um **prompt de sistema** que define o contexto e as regras. Por exemplo, o agente Augment utiliza uma mensagem inicial (inspirada no prompt do Anthropic Claude) semelhante a:

```text
“Você é um assistente para codificação dentro de um projeto Python. Foi carregado o repositório em {caminho}. A seguir está a descrição de um Pull Request (tarefa) a implementar. **Seu objetivo é fazer mudanças mínimas nos arquivos de código (não nos testes) para atender aos requisitos descritos.** Siga estes passos:
1. Explore o repositório para se familiarizar com a estrutura.
2. Crie um script de reprodução do bug ou cenário e execute-o usando a ferramenta Bash (ex: `python repro.py`) para ver o erro atual.
3. Use a ferramenta de 'pensamento sequencial' (*sequential_thinking*) para planejar a correção – pense em 5-7 possíveis causas do problema e identifique as mais prováveis.
4. Edite o código fonte do repositório para resolver o problema.
5. Reexecute o script de reprodução e confirme que o erro foi corrigido.
6. Considere casos de borda e certifique-se de cobrir esses cenários na solução.
7. Rode alguns testes do repositório para garantir que sua correção não quebrou nada existente.” ([#1 open-source agent on SWE-Bench Verified by combining Claude 3.7 and O1](https://www.augmentcode.com/blog/1-open-source-agent-on-swe-bench-verified-by-combining-claude-3-7-and-o1#:~:text=1,that%20the%20error%20is%20fixed)) 
```

Este exemplo orienta o modelo por etapas lógicas, imitando a abordagem de um engenheiro humano. Note algumas **táticas de prompting** aplicadas: (a) reforça que não deve tocar em arquivos de teste, somente no código de implementação ([#1 open-source agent on SWE-Bench Verified by combining Claude 3.7 and O1](https://www.augmentcode.com/blog/1-open-source-agent-on-swe-bench-verified-by-combining-claude-3-7-and-o1#:~:text=Can%20you%20help%20me%20implement,the%20tests%20in%20any%20way)); (b) enfatiza “mudanças mínimas” para resolver o bug (evitando alterações desnecessárias); (c) instrui explicitamente a usar ferramentas (bash, busca, etc.) durante o processo; (d) incentiva reflexão estruturada através de um *tool* de planejamento (mais sobre isso adiante).

**Chaining e pensamento passo-a-passo:** Modelos modernos como Claude 3.7 e GPT-4 têm capacidades de raciocínio contextual que podem ser exploradas. Uma técnica eficaz é o *chaining*, onde o agente divide internamente a tarefa em sub-tarefas ou “pensamentos” sequenciais. Por exemplo, a Anthropic introduziu um modo de *extended thinking* no Claude Sonnet, e no caso do SWE-bench usou um “ferramenta de planejamento” para ajudar o modelo a **pensar em etapas antes de agir** ([#1 open-source agent on SWE-Bench Verified by combining Claude 3.7 and O1](https://www.augmentcode.com/blog/1-open-source-agent-on-swe-bench-verified-by-combining-claude-3-7-and-o1#:~:text=To%20supplant%20Anthropic%E2%80%99s%20unpublished%20%E2%80%9Cplanning%E2%80%9D,not%20the%20bottleneck%20%E2%80%93%20%E2%80%9Cgrep%E2%80%9D)). Como a ferramenta proprietária não estava disponível, a Augment substituiu por um módulo open-source chamado **Sequential Thinking MCP** – basicamente um servidor que permite ao LLM gerar uma sequência de pensamentos numerados, revisá-los, ramificar hipóteses e só então partir para a edição do código ([#1 open-source agent on SWE-Bench Verified by combining Claude 3.7 and O1](https://www.augmentcode.com/blog/1-open-source-agent-on-swe-bench-verified-by-combining-claude-3-7-and-o1#:~:text=To%20supplant%20Anthropic%E2%80%99s%20unpublished%20%E2%80%9Cplanning%E2%80%9D,not%20the%20bottleneck%20%E2%80%93%20%E2%80%9Cgrep%E2%80%9D)) ([#1 open-source agent on SWE-Bench Verified by combining Claude 3.7 and O1](https://www.augmentcode.com/blog/1-open-source-agent-on-swe-bench-verified-by-combining-claude-3-7-and-o1#:~:text=GUIDE%20FOR%20HOW%20TO%20USE,in%20between%20thoughts)). Esse “modo reflexão” guiado provou ser útil para abordar problemas complexos de forma estruturada, simulando um brainstorm interno do agente. Em seu agente, você pode integrar algo similar: por exemplo, após carregar o contexto do bug, inserir no prompt algo como *“Use a ferramenta `#pensar` para elaborar um plano de solução antes de modificar o código”*. Em runtime, o agente então chamaria a ferramenta *sequential_thinking*, que registraria passo a passo o raciocínio do modelo (potencialmente oculto do usuário ou exibido para transparência).

Outra estratégia de chaining é **dividir o trabalho em fases distintas** de prompt. Por exemplo, ter um passo de “Orientação” onde o agente apenas lê o repositório e descobre como rodar os testes ou reproduzir o bug; depois um passo de “Implementação” para aplicar a correção; e possivelmente um passo final de “Verificação” para validar e limpar a solução. Em teoria isso soa promissor, mas a experiência da Augment mostrou que nem sempre sub-dividir assim traz ganhos claros – eles tentaram adicionar um agente separado só para “fixar regressões” após a implementação e acabaram vendo que ele às vezes estragava soluções que já estavam corretas, resultando em nenhum ganho líquido ([#1 open-source agent on SWE-Bench Verified by combining Claude 3.7 and O1](https://www.augmentcode.com/blog/1-open-source-agent-on-swe-bench-verified-by-combining-claude-3-7-and-o1#:~:text=We%20explored%20some%20techniques%20to,improvement%20to%20the%20final%20score)). Portanto, é recomendável iterar com cautela: comece com um único agente/prompt capaz de ir do início ao fim, e só introduza subdivisões se perceber falhas recorrentes que poderiam ser isoladas.

**ReAct e estilo de pensamento do agente:** Internamente, muitas implementações seguem o padrão *ReAct* (Reason and Act): o agente alterna entre “pensar” (gerar um raciocínio ou planejar próxima ação) e “agir” (invocar uma ferramenta ou produzir uma resposta). O VSCode agent mode, por exemplo, permite que no meio da resposta o modelo solicite ações prefixando comandos (como `#execute` para rodar algo, `#open` para abrir arquivo) e depois retorne ao fluxo ([VS Code v1.99 Is All About Copilot Chat AI, Including Agent Mode -- Visual Studio Magazine](https://visualstudiomagazine.com/Articles/2025/04/04/VS-Code-v1-99-Is-All-About-Copilot-Chat.aspx#:~:text=,documentation)). Ao construir seu agente, você pode implementar um loop que processa a saída do modelo linha a linha ou por tokens especiais para detectar intenções de ação. Por exemplo, se o modelo responde com: *“# Terminal: python tests/run_all.py”*, o orquestrador deve capturar isso, pausar a geração, executar o comando, pegar o resultado (saída dos testes) e anexar ao prompt contínuo, sinalizando ao modelo que a ação foi concluída. Esse ciclo continua até o modelo emitir uma solução final ou atingir algum critério de parada.

Em termos de prompt design, recomenda-se também:
- **Exemplos de uso de ferramentas**: Inclua no prompt de sistema exemplos explícitos de como chamar ferramentas. Ex: *“Para buscar algo no código, escreva: `# Search "termo"`”*. Ensinar o modelo a usar corretamente suas ferramentas aumenta a eficácia (reduz respostas alucinadas quando ele deveria ter buscado no código, por exemplo).
- **Limitar instruções irrelevantes**: Forneça guidelines claras para evitar que o modelo divague fora da tarefa. Ex: “Concentre-se apenas em detalhes técnicos do código; não discuta coisas fora do escopo do PR.”.
- **Tamanho do contexto**: Para tarefas longas, talvez seja necessário truncar ou resumir partes do histórico para não estourar o limite de tokens do modelo. Utilize resumos de conversas anteriores ou da própria sequência de pensamento.

Em suma, **encadear o processo de resolução em passos e guiar o modelo por prompts detalhados** são técnicas vitais. Conforme observado, a otimização de prompts tem efeito importante até certo ponto – Augment relata que ajustar prompts trouxe melhorias, mas saturou, depois o que mais conta é o modelo em si ([#1 open-source agent on SWE-Bench Verified by combining Claude 3.7 and O1](https://www.augmentcode.com/blog/1-open-source-agent-on-swe-bench-verified-by-combining-claude-3-7-and-o1#:~:text=We%20found%20that%20scores%20on,outcomes%20between%20the%20two%20rollouts)). Portanto, encontre um bom prompt base e concentre esforços também em melhorar o modelo e os dados de contexto (a seguir).

## 3. Filtros, Memória e Heurísticas para Navegar o Código Fonte

Uma grande diferença entre gerar um simples snippet e atuar como um “engenheiro de software virtual” é a necessidade de **entender e navegar em bases de código reais**. Para isso, seu agente precisará de mecanismos de filtragem de contexto, memória de código e heurísticas inteligentes para focar no que importa em cada projeto.

**Indexação e Recuperação de Contexto (RAG):** Uma técnica essencial é implementar **RAG (Retrieval-Augmented Generation)** para código. Em vez de tentar fornecer todo o repositório no prompt (o que é inviável), a ideia é indexar previamente o código fonte e permitir que o agente consulte esse índice de forma relevante. Ferramentas líderes já fazem isso: o Continue, por exemplo, **indexa automaticamente todo o workspace combinando busca por palavra-chave com embeddings semânticos**, armazenando os embeddings localmente para preservar privacidade ([@Codebase | Continue](https://docs.continue.dev/customize/deep-dives/codebase#:~:text=Continue%20indexes%20your%20codebase%20so,continue%2Findex)). Assim, quando o usuário faz uma pergunta envolvendo `@Codebase`, o Continue consegue recuperar trechos de arquivos similares ao query e fornecê-los ao modelo como contexto ([@Codebase | Continue](https://docs.continue.dev/customize/deep-dives/codebase#:~:text=Continue%20indexes%20your%20codebase%20so,continue%2Findex)). Você pode reproduzir esse recurso usando bibliotecas em Node.js: por exemplo, usar o `transformers.js` (porta do HuggingFace Transformers para JS) para gerar embeddings de cada função/arquivo, e armazená-los em um banco vetorial local (poderia ser simples, como um ANN em memória, ou algo como LanceDB ou Milvus rodando local). Em runtime, ao receber a descrição do problema, extrair termos chave (nomes de classes, funções, mensagens de erro) e **consultar o índice vetorial + textual** para obter os N trechos de código mais relevantes. Esses trechos então são inseridos no prompt para o modelo. Essa abordagem garante que mesmo um modelo menor consiga “ler” partes importantes de um grande códigobase sem ter que ingerir tudo.

- *Exemplo:* Se a tarefa é “consertar o bug na validação de usuário duplicado”, o agente pode buscar por palavras como “duplicate”, “user”, “validate” no repositório (busca textual) e também usar a representação vetorial do enunciado do bug para encontrar funções com semântica similar (busca por embeddings). O resultado pode ser, por exemplo, identificar que o arquivo **`user_service.py`** e a função **`check_user_exists()`** aparecem como altamente correlatos. Esses arquivos/trechos então são agregados no prompt como `<context>` adicional antes da geração da solução.

**Heurísticas de seleção de arquivo:** Além de busca, pode-se aplicar heurísticas simples baseadas em estrutura do projeto. Por exemplo: se a descrição do problema mencionar “endpoint” ou “API”, talvez concentrar na pasta `controllers/` ou `routes/`. Se mencionar erro em teste, parsear o log do teste falho (usando regex para pegar nome de função/arquivo no stack trace). Outra heurística usada em agentes de código é priorizar **arquivos recentemente modificados** (no caso de iterações) – se numa tentativa anterior o agente editou um arquivo, na próxima iteração esse arquivo tem alta chance de ainda ser relevante (ou de conter o bug se não resolveu totalmente).

No SWE-bench em particular, constatou-se que simples ferramentas de busca como `grep` e `find` já eram surpreendentemente úteis para navegação ([#1 open-source agent on SWE-Bench Verified by combining Claude 3.7 and O1](https://www.augmentcode.com/blog/1-open-source-agent-on-swe-bench-verified-by-combining-claude-3-7-and-o1#:~:text=We%20were%20also%20impressed%20by,bench)). O agente da Anthropic/Augment frequentemente faz chamadas como `grep "palavra"` para localizar onde no código certo termo aparece, ou `find . -name "*.py"` para entender a estrutura de diretórios. Essa abordagem orientada a texto funciona bem em projetos pequenos/médios (e no benchmark, que foca Python). Entretanto, **em bases de código muito extensas e com requisitos ambíguos, é preciso ir além** – Augment notou que no produto real, utilizar embeddings e indexação é **crítico** para qualidade, enquanto grep puro enfrenta limitações ([#1 open-source agent on SWE-Bench Verified by combining Claude 3.7 and O1](https://www.augmentcode.com/blog/1-open-source-agent-on-swe-bench-verified-by-combining-claude-3-7-and-o1#:~:text=We%20were%20also%20impressed%20by,bench)) ([#1 open-source agent on SWE-Bench Verified by combining Claude 3.7 and O1](https://www.augmentcode.com/blog/1-open-source-agent-on-swe-bench-verified-by-combining-claude-3-7-and-o1#:~:text=experimented%20with%20variations%20to%20the,deliver%20a%20great%20product%20experience)). Portanto, implemente ambos: tenha ferramentas de busca literal (mais rápidas e diretas) e de busca semântica (para casos em que a referência não é explícita).

**Memória do Agente:** Além do contexto técnico do código, há a *memória conversacional* e possivelmente *memória a longo prazo*. Memória conversacional é simplesmente manter as últimas interações no prompt para continuidade (por ex., histórico do chat com o usuário e pensamentos do agente). Você pode resumir ou truncar conforme necessário, mas isso garante que o agente lembre o que já tentou ou o que o usuário corrigiu. Já memória de longo prazo pode envolver o agente **aprender com interações passadas** no projeto: por exemplo, se em uma sessão anterior o desenvolvedor corrigiu manualmente um erro ou deu um feedback “essa abordagem não funciona por causa X”, o agente poderia armazenar isso (em arquivo local JSON, banco, etc.) e na próxima vez evitar o mesmo erro. Augment cita que seus agentes de produção **memorizaram feedback e dicas dos desenvolvedores para melhorar ao longo do tempo**, algo não avaliado no SWE-bench mas valioso em uso real ([#1 open-source agent on SWE-Bench Verified by combining Claude 3.7 and O1](https://www.augmentcode.com/blog/1-open-source-agent-on-swe-bench-verified-by-combining-claude-3-7-and-o1#:~:text=Real,bench%20%28in%20its%20current%20form)). Implementar isso poderia ser um diferencial: por exemplo, registrar os diffs aprovados vs. rejeitados e eventualmente fine-tunar o modelo do agente com esses dados (ver seção de melhoria contínua).

**Filtragem e Regras de Segurança:** É importante definir filtros para evitar ações indesejadas. Por exemplo, no prompt do agente inclua regras como “Não delete arquivos sem autorização”, “Não chame ferramentas externas não autorizadas (como `rm -rf`)”, etc. O VSCode Agent Mode exige confirmação do usuário para ações potencialmente destrutivas ou de escrita ([Agent mode: available to all users and supports MCP](https://code.visualstudio.com/blogs/2025/04/07/agentMode#:~:text=Image%3A%20Edit%20UI%20showing%20how,to%20enable%20and%20disable%20tools)) – seu agente também deve ter esse cuidado. Implemente níveis de permissão: talvez permitir leitura de arquivos automaticamente, mas pedir aprovação para escrever no disco ou rodar um script. Assim, o desenvolvedor fica tranquilo para delegar tarefas sem arriscar o agente corromper algo crítico.

Outra forma de filtro é **validar a saída do modelo antes de aplicar**. Ex: se o modelo sugeriu um patch de código, você pode verificar se o patch realmente aplica limpo (usando `patch` dry-run) e se modifica apenas arquivos permitidos. No caso do Augment, eles inseriram explicitamente no prompt que **o agente não deve modificar arquivos de teste** ([#1 open-source agent on SWE-Bench Verified by combining Claude 3.7 and O1](https://www.augmentcode.com/blog/1-open-source-agent-on-swe-bench-verified-by-combining-claude-3-7-and-o1#:~:text=Can%20you%20help%20me%20implement,the%20tests%20in%20any%20way)), forçando-o a se concentrar na lógica. Esse tipo de restrição de escopo ajuda a evitar danos e a manter o agente focalizado em consertar o código fonte (e não trapacear alterando testes para passarem). 

**Resumindo heurísticas e memória:**

- Use *embeddings* e *keyword search* para recuperar contexto relevante de código ([@Codebase | Continue](https://docs.continue.dev/customize/deep-dives/codebase#:~:text=Continue%20indexes%20your%20codebase%20so,continue%2Findex)).
- Use ferramentas de busca (grep/find) integradas para navegação rápida ([#1 open-source agent on SWE-Bench Verified by combining Claude 3.7 and O1](https://www.augmentcode.com/blog/1-open-source-agent-on-swe-bench-verified-by-combining-claude-3-7-and-o1#:~:text=We%20were%20also%20impressed%20by,bench)).
- Parseie mensagens de erro e stack traces automaticamente para apontar o agente na direção certa.
- Mantenha histórico de conversação para consistência e evite repetir ações inúteis.
- Imponha regras de filtro no prompt (ou no código do agente) para restringir ações (ex.: não modificar certos diretórios, não usar internet a menos que permitido, etc).
- Opcionalmente, incorpore **resumos do código**: gerar sumários de arquivos ou diagramas de classes pode ajudar o modelo se o código for muito extenso. Uma ferramenta útil é gerar documentação automática (e.g., usando uma ferramenta AST para extrair docstrings, ou até pedir para o modelo resumir um arquivo grande em poucas frases e reutilizar esse sumário posteriormente).

Com um gerenciamento inteligente de contexto e memória, seu agente será capaz de escalar para bases de código consideravelmente maiores do que o limite de tokens do modelo, navegando de forma inteligente. 

## 4. Ferramentas e Bibliotecas Recomendadas

Construir um agente complexo do zero pode ser reinventar a roda. Felizmente, há frameworks e bibliotecas que facilitam tanto a integração com LLMs quanto a orquestração de agentes e ferramentas. Abaixo listamos algumas recomendações:

- **LangChain**: Biblioteca popular para construção de aplicações com LLMs. Possui versões em Python e JavaScript/TypeScript. No contexto Node.js (VSCode extension), você pode usar o *LangChain.js*. Ele fornece abstrações para **Chains** (encadeamento de prompts), **Agents** (que decidem ações com base em ferramentas) e **Memory**. Por exemplo, há implementações de agentes estilo ReAct onde você registra ferramentas (como funções JS para ler arquivo, executar comando) e o LangChain cuida de inserir no prompt a escolha de ferramenta e analisar a resposta do modelo. O extension **AgentSmith** do VSCode demonstra o uso de LangChain para customizar agentes diretamente no editor – ele permite criar ferramentas e agentes via configuração YAML ou código, integrando múltiplos LLMs facilmente ([
        Agent Smith - Visual Studio Marketplace
    ](https://marketplace.visualstudio.com/items?itemName=nr-codetools.agentsmith#:~:text=%E2%9B%93%20Key%20Features%3A)). Se preferir não depender de LangChain, ao menos estude sua arquitetura pois ela inspira soluções: por exemplo, como formatar a saída do modelo para indicar ações (LangChain usa o formato “[Action]: [Input]”) e como estruturar a interação agente<->ferramenta.

- **Model Context Protocol (MCP)**: Conforme mencionado, o MCP é um protocolo emergente para conectar agentes a IDEs e a diversos **servidores de ferramentas**. A própria Microsoft/VSCode mantém um repositório oficial de servidores MCP (em diversas linguagens) que implementam ferramentas úteis – por exemplo, há um servidor MCP para integração com GitHub Issues, outro para buscar respostas na web (Perplexity AI), outro para desenhar diagramas Mermaid ([Agent mode: available to all users and supports MCP](https://code.visualstudio.com/blogs/2025/04/07/agentMode#:~:text=%2A%20Built,green%20in%20the%20diagram)) ([Agent mode: available to all users and supports MCP](https://code.visualstudio.com/blogs/2025/04/07/agentMode#:~:text=,green%20in%20the%20diagram)). Você pode aproveitar esses servidores existentes em vez de implementar tudo. Por exemplo, se quiser que seu agente possa obter as issues do projeto, basta registrar o MCP do GitHub. Para usar MCP no Node, verifique o projeto `modelcontextprotocol/servers` no GitHub – há implementações em TypeScript (como o *sequential_thinking* que citamos ([#1 open-source agent on SWE-Bench Verified by combining Claude 3.7 and O1](https://www.augmentcode.com/blog/1-open-source-agent-on-swe-bench-verified-by-combining-claude-3-7-and-o1#:~:text=To%20supplant%20Anthropic%E2%80%99s%20unpublished%20%E2%80%9Cplanning%E2%80%9D,not%20the%20bottleneck%20%E2%80%93%20%E2%80%9Cgrep%E2%80%9D))). O VSCode facilita adicionar MCP servers via arquivo de configuração (`.vscode/mcp.json`) ou configurações da extensão ([servers/src/sequentialthinking at main · modelcontextprotocol/servers · GitHub](https://github.com/modelcontextprotocol/servers/tree/main/src/sequentialthinking#:~:text=Usage%20with%20Claude%20Desktop)) ([servers/src/sequentialthinking at main · modelcontextprotocol/servers · GitHub](https://github.com/modelcontextprotocol/servers/tree/main/src/sequentialthinking#:~:text=%22args%22%3A%20%5B%20%22,)), e se sua extensão declarar dependência deles, o agente poderá chamá-los como ferramentas. Resumindo: **MCP + Extensões VSCode** são a forma oficial de estender agentes; use-os para recursos como busca web, geração de imagens, etc., sem precisar codificar do zero.

- **Frameworks de Multi-agentes (CrewAI, AutoGen)**: Para ir além de um único agente, frameworks como **CrewAI** e **AutoGen** (da Microsoft) fornecem infraestrutura para vários agentes trabalhando em conjunto. O CrewAI, por exemplo, é open-source (Python) e permite orquestrar agentes com diferentes papéis e até **delegar tarefas entre si** ([CrewAI vs AutoGen for Code Execution AI Agents — E2B Blog](https://e2b.dev/blog/crewai-vs-autogen-for-code-execution-ai-agents#:~:text=CrewAI%20is%20built%20on%20top,delegate%20work%20to%20each%20other)). Ele é construído sobre o LangChain e traz exemplos de agentes que escrevem código e outros que executam/verificam. Já o **AutoGen** (Microsoft) foca em agentes capazes de gerar e **executar código em Docker** para ver o resultado ([CrewAI vs AutoGen for Code Execution AI Agents — E2B Blog](https://e2b.dev/blog/crewai-vs-autogen-for-code-execution-ai-agents#:~:text=AutoGen)) – semelhante a ter um agente programador e um agente executor/tester. Esses frameworks podem servir de inspiração: mesmo que você implemente em Node, pode adotar ideias como *“Criar um agente Validador que roda testes em paralelo”*. No GitHub da Microsoft Research há um projeto `autogen` com agentes conversando via mensagens (um User proxy e um Assistant e um Python executor). Vale a leitura para entender padrões de coordenação. Em Node, você pode implementar multi-agentes simplesmente instanciando mais de um LLM com personalidades distintas e fazendo-os trocarem mensagens via seu orquestrador.

- **vscode-languageserver / LSP**: Caso deseje que seu agente tenha funcionalidades de linguagem (tipo autocompletar, realçar erros) ou queira uma arquitetura cliente-servidor robusta, considere implementar o backend como um Language Server. O pacote `vscode-languageserver` facilita criar um processo Node que se comunica com o editor via JSON-RPC. Apesar do LSP não ter sido projetado para agentes autônomos, você pode usá-lo para, por exemplo, aproveitar notificações de abertura/fechamento de arquivos, ou enviar diagnósticos (mensagens de erro) para o editor. Por outro lado, a chegada do Agent Mode com MCP torna possível integrar sem usar LSP diretamente, então avalie o custo-benefício. Se seu agente for bem específico a uma língua (ex: só Python), um LSP custom que implementa requests específicos (como “corrigirErro”) poderia ser interessante.

- **Outras bibliotecas e recursos úteis**:
  - *Tree-sitter*: Biblioteca para parsing de código em várias linguagens. Pode ser usada para dividir código em funções, classes e extrair estrutura sintática. Útil na etapa de indexação de código (pode melhorar a segmentação para embeddings, por exemplo armazenar embedding por função).
  - *LanceDB ou Milvus*: bancos de dados vetoriais que podem rodar localmente para armazenar embeddings e fazer busca k-NN rápida.
  - *Ollama*: serve modelos como LLaMA 2 localmente via uma API simples. Pode ser integrado para rodar o modelo local isolado do processo do VSCode (reduzindo risco de travamento por OOM na extensão).
  - *Transformers.js*: como mencionado, se quiser evitar spin-up de Python, essa lib permite rodar alguns modelos de HF Transformers diretamente em Node (via WebAssembly). Continuedev usa para calcular embeddings localmente ([@Codebase | Continue](https://docs.continue.dev/customize/deep-dives/codebase#:~:text=Continue%20indexes%20your%20codebase%20so,continue%2Findex)). Você pode usá-la também para inferência de modelos menores (embora para um agente complexo, provavelmente melhor usar um backend dedicado para o LLM principal).
  - *OpenAI/Anthropic SDKs*: Para chamadas a APIs externas, use as SDKs oficiais (openai-node, etc.) ou clientes HTTP robustos. Lembre de implementar timeouts e retentativas, pois chamadas de LLM podem ser lentas ou falhar ocasionalmente.
  - *Ferramentas de Diff*: Para aplicar mudanças no código, uma abordagem conveniente é o agente gerar um diff unificado (patch) em vez de arquivo completo. Você pode usar libs como `diff` do Node para aplicar ou validar patches. Alguns agentes (Cursor, Copilot) trabalham no nível de “blocos editáveis” – você pode delinear no prompt que ele deve outputar por exemplo: `«BEGIN_DIFF» ... «END_DIFF»` e então seu extensão aplica o diff automaticamente no workspace.

Em termos de bibliotecas prontas, o ecossistema está evoluindo rápido. O importante é escolher componentes que **se encaixem no ambiente Node/VSC** e que sejam flexíveis para personalização. Continue.dev, por exemplo, optou por criar um formato YAML declarativo para configurar agentes (modelos, regras, ferramentas) ([Launch HN: Continue (YC S23) – Create custom AI code assistants | Hacker News](https://news.ycombinator.com/item?id=43494427#:~:text=The%20AI,locking%20you%20into%20yesterday%27s%20technology)) ([Launch HN: Continue (YC S23) – Create custom AI code assistants | Hacker News](https://news.ycombinator.com/item?id=43494427#:~:text=The%20AI,locking%20you%20into%20yesterday%27s%20technology)) – isso é algo que você também pode expor para usuários avançados personalizarem seu agente sem precisar mexer em código.

## 5. Estratégias de Avaliação e Melhoria Contínua

Alcançar >60% no SWE-bench não é trivial – exige refinar o agente iterativamente. Para isso, é fundamental estabelecer processos de avaliação e feedback contínuo:

- **Benchmarking Automatizado (SWE-bench)**: Utilize o próprio SWE-bench como prova de fogo para seu agente durante o desenvolvimento. A Augment open-sourçou um pipeline completo que executa o agente em cada problema do benchmark dentro de contêiner Docker e avalia se os testes passaram ([#1 open-source agent on SWE-Bench Verified by combining Claude 3.7 and O1](https://www.augmentcode.com/blog/1-open-source-agent-on-swe-bench-verified-by-combining-claude-3-7-and-o1#:~:text=Check%20out%20our%20open,containers%2C%20ensembling%2C%20evaluating%20candidate%20solutions)). Você pode se inspirar nessa configuração: por exemplo, ter um script que pega cada caso do SWE-bench (um repositório + descrição de issue), roda seu agente até ele propor uma solução (dif no código), aplica a solução num container isolado e verifica os testes. Isso lhe dará uma métrica quantitativa (percentual de sucesso) para acompanhar o progresso. Integre isso no CI do seu projeto – rodar periodicamente e ver tendências.

- **Logging e Análise de Falhas:** Ative logs detalhados do agente durante as execuções de teste. Grave cada passo: prompt enviado, resposta do modelo, ações tomadas, resultado de comandos, etc. Com esses logs, analise manualmente os casos onde o agente falhou. Pergunte: Ele escolheu o arquivo errado? Alucinou uma função que não existe? Esqueceu de rodar os testes de regressão? Esse diagnóstico é ouro para saber onde melhorar. Por exemplo, se notar que em 5 casos ele não conseguiu entender como iniciar o projeto (talvez faltou ler o README), você pode ajustar o prompt para sempre ler o README primeiro ou adicionar uma ferramenta para isso. Se ele frequentemente deixa passar um edge case que quebra um teste oculto, talvez precise enfatizar passo 6 (pensar em edge cases) no prompt.

- **Modelo e Fine-tuning:** Conforme mencionado, a **qualidade do modelo base é o principal fator de performance** no SWE-bench ([#1 open-source agent on SWE-Bench Verified by combining Claude 3.7 and O1](https://www.augmentcode.com/blog/1-open-source-agent-on-swe-bench-verified-by-combining-claude-3-7-and-o1#:~:text=We%20found%20that%20scores%20on,outcomes%20between%20the%20two%20rollouts)). Modelos como Claude 3.7 ou GPT-4 têm desempenho muito superior a LLMs open-source menores. Se seu objetivo é superar o Augment, você provavelmente precisará ou **usar modelos de ponta** ou **treinar/fintunar um modelo ao nível deles**. Como preferimos local-first, uma opção é pegar o melhor modelo open disponível (digamos, um LLaMA 3 ou 4 se lançado, ou um Code Llama 2 de 34B) e realizar *fine-tuning* específico para tarefas de bug-fixing e entendimento de código. Você pode utilizar as próprias tarefas do SWE-bench (que são issues reais do GitHub) como dado de fine-tune (embora cuidado para não viciar no benchmark exato, use problemas similares adicionais se possível). Além disso, **Reforço com Feedback Humano (RLHF)** pode aumentar a robustez: por exemplo, após algumas utilizações reais, coletar exemplos de onde o agente foi bem ou mal e treinar uma recompensa ou diretamente ajustar o modelo. A Augment indicou que está investindo em **fine-tuning de modelos open com aprendizado por reforço e dados proprietários** para melhorar latência e custo mantendo performance do benchmark ([#1 open-source agent on SWE-Bench Verified by combining Claude 3.7 and O1](https://www.augmentcode.com/blog/1-open-source-agent-on-swe-bench-verified-by-combining-claude-3-7-and-o1#:~:text=Given%20these%20learnings%2C%20we%20think,from%20us%20on%20this%20point)). Seguir esse caminho envolve esforço (montar pipeline de RLHF, ter humanos avaliando respostas), mas pode render um agente altamente especializado que supera mesmo os modelos fechados em certos nichos.

- **Ensembling e Auto-Retries:** Técnicas de ensemble (rodar várias tentativas e escolher a melhor) provaram elevar a pontuação em alguns pontos percentuais ([#1 open-source agent on SWE-Bench Verified by combining Claude 3.7 and O1](https://www.augmentcode.com/blog/1-open-source-agent-on-swe-bench-verified-by-combining-claude-3-7-and-o1#:~:text=We%20found%20that%20scores%20on,outcomes%20between%20the%20two%20rollouts)). Por exemplo, a Anthropic no modo “high compute” rodou ~5 tentativas paralelas e escolheu a melhor com um modelo julgador, atingindo ~70% ([Claude 3.7 Sonnet and Claude Code \ Anthropic](https://www.anthropic.com/news/claude-3-7-sonnet#:~:text=,best%20one%20for%20the%20submission)). Augment testou um ensemble simples via *majority vote* usando um segundo modelo (OpenAI O1) para escolher entre múltiplas soluções ([#1 open-source agent on SWE-Bench Verified by combining Claude 3.7 and O1](https://www.augmentcode.com/blog/1-open-source-agent-on-swe-bench-verified-by-combining-claude-3-7-and-o1#:~:text=For%20ensembling%2C%20we%20used%20a,world%20settings)). O ganho foi modesto (3-8%) ([#1 open-source agent on SWE-Bench Verified by combining Claude 3.7 and O1](https://www.augmentcode.com/blog/1-open-source-agent-on-swe-bench-verified-by-combining-claude-3-7-and-o1#:~:text=We%20found%20that%20scores%20on,outcomes%20between%20the%20two%20rollouts)), mas real. Você pode implementar algo semelhante: rodar o agente N vezes com variação estocástica (alterando seed ou temperatura) e depois ter um módulo que analisa as diferentes soluções. Esse “módulo de votação” poderia ser outro LLM (um modelo especializado em avaliar diffs ou outputs de teste) ([#1 open-source agent on SWE-Bench Verified by combining Claude 3.7 and O1](https://www.augmentcode.com/blog/1-open-source-agent-on-swe-bench-verified-by-combining-claude-3-7-and-o1#:~:text=For%20ensembling%2C%20we%20used%20a,world%20settings)) ou regras simples (ex: priorizar a solução que passou mais testes visíveis). Contudo, tenha em mente custo e tempo – rodar múltiplas vezes multiplica o consumo de tokens e demora. Talvez faça sentido apenas como opção “try again” do usuário ou para avaliação offline. Em produção interativa, geralmente uma boa execução já basta, com possibilidade de refinar se falhar.

- **Avaliação de Qualidade do Código:** Além de passar nos testes, interessa também a qualidade da correção (legibilidade, aderência aos padrões). Ferramentas de análise estática e linters podem ser integradas no pipeline: depois que o agente propõe uma solução, rode um linter/formatter e veja se algo bizarro aparece (ex: variáveis não usadas, warnings). Isso pode ser alimentado de volta ao modelo para melhorar a resposta antes de mostrar ao usuário. Outra ideia: usar um modelo crítico (como GPT-4) para fazer *code review* da saída do agente e apontar possíveis problemas. Esse modelo crítico poderia funcionar como um juiz que valida se a solução parece razoável (esta técnica é análoga ao “scoring model” que Anthropic mencionou usar para rankear soluções ([Claude 3.7 Sonnet and Claude Code \ Anthropic](https://www.anthropic.com/news/claude-3-7-sonnet#:~:text=,best%20one%20for%20the%20submission))). Em termos concretos, você poderia ter um prompt para GPT-4 dizendo “Aqui está a descrição do problema e um diff de solução proposto. Avalie se a solução cumpre os requisitos e não introduz problemas.” e usar a resposta para decidir aprovar ou pedir nova tentativa.

- **Aprendizado Contínuo:** Após implementar seu agente, planeje ciclos de melhoria contínua. Por exemplo, a cada nova semana, reavalie no SWE-bench e compare resultados. Observe se há categorias de problemas em que ele vai mal (ex: todos bugs de concorrência falham). Colete também feedback de usuários reais se disponível: logs de conversas onde o agente não atendeu bem o pedido. Com esse corpus de falhas, realize *fine-tuning incremental*: crie prompts de treinamento que ensinem o comportamento correto nas situações problemáticas. Isso pode ser via **Supervised Fine-Tuning (SFT)** (modelo aprende a mapear do input para a resposta esperada fornecida por você ou por um especialista), ou via **Reinforcement Learning** onde você define uma recompensa alta para passar nos testes e treina o agente a maximizar isso (mais complexo de implementar, pois requer um ambiente simulável e algorítmos tipo PPO).

- **Considerar Outras Métricas:** O SWE-bench é um ótimo indicador, porém Augment e outros ressaltam que nenhum benchmark captura tudo ([#1 open-source agent on SWE-Bench Verified by combining Claude 3.7 and O1](https://www.augmentcode.com/blog/1-open-source-agent-on-swe-bench-verified-by-combining-claude-3-7-and-o1#:~:text=codebase%20awareness%20and%20navigation,by%20an%20experienced%20software%20engineer)) ([#1 open-source agent on SWE-Bench Verified by combining Claude 3.7 and O1](https://www.augmentcode.com/blog/1-open-source-agent-on-swe-bench-verified-by-combining-claude-3-7-and-o1#:~:text=Real,bench%20%28in%20its%20current%20form)). Portanto, olhe além da taxa de sucesso bruta: meça também o tempo médio para resolver um problema, o número de interações ou perguntas feitas ao usuário (menos é melhor), etc. Uma métrica interna útil pode ser **“autonomia média”** – ex.: em quantos casos o agente resolveu sem precisar de dica humana. Isso importa porque um agente que atinge 70% mas sempre requer pequenas intervenções talvez não seja tão “autônomo” quanto se pensa.

Por fim, **leverage the community**: compartilhe resultados, peça outros para testarem no repositório deles. Muitas ideias de melhoria vêm de observar usos variados. Como o campo é novo, cada experimento – seja um prompt melhor ou um novo tool – pode render ganhos significativos.

## 6. Multi-agentes Especializados e Delegação de Tarefas

Para alcançar desempenho superior e robustez, uma estratégia promissora é incorporar **múltiplos agentes cooperando**, cada um focado em uma sub-tarefa da engenharia de software. Em vez de um único agente tentar fazer tudo, podemos ter especialistas que se comunicam entre si, sob sua coordenação. Veja algumas possibilidades e exemplos:

- **Agente Desenvolvedor + Agente Revisor (PR-Agent)**: Uma configuração simples de dois agentes pode espelhar o fluxo de trabalho de desenvolvimento normal – um agente “desenvolvedor” gera a correção do código, enquanto um agente “revisor” (inspiração no **PR-Agent** do Qodo/CodiumAI) analisa a mudança e dá feedback ([PR-Agent (Qodo Merge open-source): An AI-Powered Tool ... - GitHub](https://github.com/qodo-ai/pr-agent#:~:text=GitHub%20github.com%20%20PR,providing%20AI%20feedback%20and%20suggestions)) ([Enhancing Code Reviews with AI PR Agent - Guangya Liu - Medium](https://gyliu513.medium.com/enhancing-code-reviews-with-ai-pr-agent-fe1c92244d51#:~:text=Enhancing%20Code%20Reviews%20with%20AI,automate%20and%20improve%20code%20reviews)). O PR-Agent real é uma ferramenta que utiliza IA para revisar pull requests, apontando erros e sugerindo melhorias automaticamente. Você pode adaptar essa ideia internamente: após o agente principal propor um patch, invocar um segundo modelo (potencialmente maior ou com prompt treinado em revisão) para inspecionar o patch. Se o revisor aprovar, ótimo; se ele encontrar problemas (“este patch não cobre tal caso” ou “quebra a função X”), então o agente desenvolvedor pode receber esse feedback e iterar. Essa dinâmica **self-review** tende a pegar falhas que um único passo ignoraria. Estudos recentes mostram que ter modelos em debate ou revisão cruzada melhora a qualidade da solução final.

- **Agente Gerenciador/Orquestrador**: Em setups de múltiplos agentes, muitas vezes se introduz um agente “gerente” ou “planner” cujo trabalho é decompor a tarefa e delegar aos agentes certos. Por exemplo, dado um bug complexo, o gerente poderia decidir: *“Preciso de um especialista em banco de dados para este erro de SQL e um especialista em UI para ajustar a validação no frontend”*. Aí invoca (ou instância) agentes especializados nessas áreas. Embora possa não ser necessário algo tão elaborado para SWE-bench (que é monolítico em Python), numa ferramenta geral essa separação por domínio pode ser útil. Vale mencionar que frameworks como o CrewAI permitem explicitamente que agentes criem novos agentes subordinados e repassem subtarefas ([CrewAI vs AutoGen for Code Execution AI Agents — E2B Blog](https://e2b.dev/blog/crewai-vs-autogen-for-code-execution-ai-agents#:~:text=CrewAI%20is%20built%20on%20top,delegate%20work%20to%20each%20other)). Essa capacidade de **delegação** pode ser explorada cuidadosamente – ex.: seu agente principal ao se deparar com uma documentação muito extensa a ler, poderia criar um agente leitor resumidor para aquele documento e depois pegar o sumário.

- **Agente de Teste/Execução**: Ter um agente dedicado a executar o código e interpretar resultados de teste também ajuda. Em vez do agente desenvolvedor interromper seu fluxo para ler saída de terminal, um agente executor pode observar logs de execução e traduzir em feedback de alta nível (*“ainda há 2 testes falhando, ambos relacionados à função X, possivelmente questão de tipo de retorno”*). Esse agente pode até sugerir hipóteses de correção que o agente desenvolvedor não pensou. AutoGen da Microsoft exemplifica isso – lá um agente “Python” executa o código e informa o agente “engenheiro” sobre exceções ou resultados ([CrewAI vs AutoGen for Code Execution AI Agents — E2B Blog](https://e2b.dev/blog/crewai-vs-autogen-for-code-execution-ai-agents#:~:text=AutoGen)).

- **Swarm de Agentes (Ensemble paralelo)**: Outra forma de multi-agente é não cooperativa mas competitiva: lance vários agentes (com variações ou inicializações diferentes) para resolver o mesmo problema em paralelo e depois escolha o melhor resultado. Isso se relaciona com o ensemble que discutimos na seção de avaliação. Um paper intitulado “More Agents Is All You Need” sugeriu que aumentar o número de agentes tentando pode escalar a performance via simples método de amostragem e votação ([CrewAI vs AutoGen for Code Execution AI Agents — E2B Blog](https://e2b.dev/blog/crewai-vs-autogen-for-code-execution-ai-agents#:~:text=A%20new%20paper%20More%20Agents,agent%20frameworks%20is%20justified)). Em contexto local, você poderia rodar duas instâncias do modelo, talvez uma com temperatura mais alta explorando alternativas e outra mais conservadora, e depois comparar soluções. No limite, se tiver recursos, um *swarm* de 5-10 agentes poderia gerar diversas soluções e um meta-agente escolhe ou combina (vota) a final. Esse tipo de abordagem eleva consumo computacional, mas se estiver buscando superar limites e tiver como paralelizar (por exemplo, usando vários threads ou uma GPU robusta), pode ser caminho para bater aquela última margem de score.

- **Especialistas de Domínio**: Considere treinar variações do agente principal especializadas em certos domínios ou tipos de tarefa. Ex: um agente perito em segurança para detectar e corrigir falhas de segurança; um agente UX para melhorias de interface; um agente documentador para escrever documentação após a implementação. Eles podem ser acionados condicionalmente. Isso foge um pouco do core de “resolver bug de código”, mas num produto completo poderia agregar valor. Por exemplo, após corrigir o bug, o agente documentador poderia atualizar o CHANGELOG ou comentários de código automaticamente.

Para implementar multi-agentes no seu projeto Node, você pode estruturar de modo que cada agente seja uma classe/modulo com seu próprio prompt inicial e possivelmente modelo distinto. Eles se comunicam via o orquestrador (que pode simplesmente intercalar mensagens, ou juntar todos num único contexto se usando um modelo com capacidade de multi-turn role-play). Ferramentas como o AgentSmith tornam essa experimentação mais fácil dentro do VSCode, oferecendo visualização de fluxo de agentes e chat com múltiplos agentes ([
        Agent Smith - Visual Studio Marketplace
    ](https://marketplace.visualstudio.com/items?itemName=nr-codetools.agentsmith#:~:text=Chat%20with%20Agents%3A%20Converse%20with,Agents%20and%20validate%20their%20responses)) ([
        Agent Smith - Visual Studio Marketplace
    ](https://marketplace.visualstudio.com/items?itemName=nr-codetools.agentsmith#:~:text=Workflow%20Visualization%3A%20Visualize%20Agent%20workflows%2C,to%20design%2C%20understand%2C%20and%20optimize)). Você poderia, por exemplo, prototipar um workflow no AgentSmith, testando conversas entre um “DevAgent” e “ReviewAgent”, e depois codificar esse fluxo fixo na sua própria extensão.

**Exemplo de Delegação:** Suponha o agente principal tenta uma solução e os testes ainda falham. Ao invés de ele tentar tudo de novo às cegas, ele pode dizer: *“Chamar Agente de Correção de Regressões para analisar falhas”*. Esse agente de regressão pega o log, identifica exatamente qual cenário quebrou e sugere um ajuste específico. O agente principal incorpora e resolve. Esse era exatamente o conceito que Augment explorou (mas implementando como fases no mesmo agente) ([#1 open-source agent on SWE-Bench Verified by combining Claude 3.7 and O1](https://www.augmentcode.com/blog/1-open-source-agent-on-swe-bench-verified-by-combining-claude-3-7-and-o1#:~:text=We%20explored%20some%20techniques%20to,improvement%20to%20the%20final%20score)) – talvez como agente separado com personalidade distinta pudesse ter melhor efeito. A chave é projetar a interface de comunicação: provavelmente a mensagem do principal para o secundário seria algo como “Tenho estes diffs e estes testes falhando, o que fazer?”. E o secundário responde com instruções ou até um patch adicional.

Por fim, lembre-se que adicionar agentes aumenta complexidade, então faça-o de forma incremental. Garanta primeiro que um agente único funcione bem; depois introduza um segundo para um papel bem justificado onde o primeiro tropeça. Cada agente extra também pode aumentar latência, então equilibre vantagem vs. custo.

---

**Referências selecionadas:** Ferramentas de IA de código líderes inspiraram este guia. A Augment Code, por exemplo, atingiu 65.4% no SWE-bench combinando o modelo Claude 3.7 com um modelo OpenAI para ensemble ([#1 open-source agent on SWE-Bench Verified by combining Claude 3.7 and O1](https://www.augmentcode.com/blog/1-open-source-agent-on-swe-bench-verified-by-combining-claude-3-7-and-o1#:~:text=that%20we%20have%20achieved%20a,published%20spot%20on%20the%20leaderboard)) ([#1 open-source agent on SWE-Bench Verified by combining Claude 3.7 and O1](https://www.augmentcode.com/blog/1-open-source-agent-on-swe-bench-verified-by-combining-claude-3-7-and-o1#:~:text=In%20short%2C%20we%20experimented%20with,model%20and%20OpenAI%E2%80%99s%20O1%20model)), usando prompting estruturado e ferramentas como pensamento sequencial e execução de testes. O Cursor e o Continue demonstram integrações profundas com IDE – realizando busca em código, preenchendo contexto via embeddings ([@Codebase | Continue](https://docs.continue.dev/customize/deep-dives/codebase#:~:text=Continue%20indexes%20your%20codebase%20so,continue%2Findex)), e operando em modo “agente autônomo” dentro do editor ([VS Code v1.99 Is All About Copilot Chat AI, Including Agent Mode -- Visual Studio Magazine](https://visualstudiomagazine.com/Articles/2025/04/04/VS-Code-v1-99-Is-All-About-Copilot-Chat.aspx#:~:text=Agent%20Mode%20offers%20an%20autonomous,tools%2C%20applications%2C%20and%20data%20sources)). Frameworks como LangChain, AutoGen e CrewAI mostram que é possível orquestrar agentes e ferramentas de forma flexível, inclusive com múltiplos agentes cooperando ([CrewAI vs AutoGen for Code Execution AI Agents — E2B Blog](https://e2b.dev/blog/crewai-vs-autogen-for-code-execution-ai-agents#:~:text=CrewAI%20is%20built%20on%20top,delegate%20work%20to%20each%20other)). Por meio das estratégias aqui detalhadas – uma arquitetura modular, prompting inteligente, gerenciamento de contexto, uso de boas bibliotecas e melhoria contínua via benchmarks – você estará equipado para construir um agente de codificação que vá além do estado-da-arte atual e realmente auxilie desenvolvedores a resolver problemas complexos de software de forma autônoma e confiável. Boa construção de agente! 

